[
{
	"uri": "https://flokkr.github.io/docs/runtime/",
	"title": "Runtime environments",
	"tags": [],
	"description": "",
	"content": "The containers could be run in different way. But the end of the day the same questions should be ansered:\n How he containers are configured? How can they locate the master components? How they are defined?  Therefore for each runtime example we include a generic descriptor table to help the comparison of various solutions.\n"
},
{
	"uri": "https://flokkr.github.io/docs/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "Flokkr documentation\n"
},
{
	"uri": "https://flokkr.github.io/docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://flokkr.github.io/docs/launcher/envtoconf/",
	"title": "Configuration transformer plugin",
	"tags": [],
	"description": "",
	"content": " Could be activated by CONFIG_TYPE=simple settings, but it\u0026rsquo;s the default.\nEvery configuration could be defined with environment variables, and they will be converted finally to hadoop xml, properties, conf or other format. The destination format (and the destination file name) is defined with the name of the environment variable according to a naming convention.\nThe generated files will be saved to the $CONF_DIR directory.\nThe source code of the converter utility can be found in a separated repository.\nNaming convention for set config keys from enviroment variables To set any configuration variable you shold follow the following pattern:\nNAME.EXTENSION_configkey=VALUE  The extension could be any extension which has a predefined transformation (currently xml, yaml, properties, configuration, yaml, env, sh, conf, cfg)\nexamples:\nCORE-SITE_fs.default.name: \u0026quot;hdfs://localhost:9000\u0026quot; HDFS-SITE_dfs_namenode_rpc-address: \u0026quot;localhost:9000\u0026quot; HBASE-SITE.XML_hbase_zookeeper_quorum: \u0026quot;localhost\u0026quot;  In some rare cases the transformation and the extension should be different. For example the kafka server.properties should be in the format key=value which is the cfg transformation in our system. In that case you can postfix the extension with an additional format specifier:\nNAME.EXTENSION!FORMAT_configkey=VALUE  For example:\nSERVER.CONF!CFG_zookeeper.address=zookeeper:2181  Available transformation  xml: HADOOP xml file format\n properties: key value pairs with : as separator\n cfg: key value pairs with = as separator\n conf: key value pairs with space as spearator (spark-defaults is an example)\n env: key value paris with = as separator\n sh: as the env but also includes the export keyword\n##### Configuration reference\nThe plugin itself could be configured with the following environment variables.\n  | Name | Default | Description | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | | CONF_DIR | Set in the docker container definitions | The location where the configuration files will be saved. | | CONFIG_TYPE | simple | For compatibility reason. If the value is simple, the conversion is active. |\n"
},
{
	"uri": "https://flokkr.github.io/docs/launcher/consul/",
	"title": "Consul config loading",
	"tags": [],
	"description": "",
	"content": " Could be activated with CONFIG_TYPE=consul\n The starter script list the configuration file names based on a consul key prefix. All the files will be downloaded from the consul key value store and the application process will be started with consul-template (enable an automatic restart in case of configuration file change)  The source code of the consul based configuration loading and launcher is available at the elek/consul-launcher repository.\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   CONFIG_TYPE consul For compatibility reason. If the value is consul, the consul based configuration handling is active.   CONSUL_PATH conf The path of the subtree in the consul where the configurations are.   CONSUL_KEY  The path where the configuration for this container should be downloaded from. The effective path will be $CONSUL_PATH/$CONSUL_KEY    BTRACE: btrace instrumentation Could be enabled with setting BTRACE_ENABLED=true or just setting BTRACE_SCRIPT.\nIt adds btrace javaagent configuration to the JAVA_OPTS (or any other opts defined by BTRACE_OPTS_VAR). The standard output is redirected to /tmp/output.log, and the btrace output will be displayed on the standard output (over a /tmp/btrace.out file)\n   Name Default Description     CONF_DIR Set in the docker container definitions The location where the configuration files will be saved.   BTRACE_SCRIPT  The location of the compiled btrace script. Coule be absolute or relative to the /opt/plugins/020_btrace/btrace   BTRACE_OPTS_VAR JAVA_OPTS The name of the shell variable where the agent parameters should be injected.    Configuration  CONSUL_PATH defines the root of the subtree where the configuration are downloaded from. The root could also contain a configuration config.ini. Default is conf\n CONSUL_KEY is optional. It defines a subdirectory to download the the config files. If both CONSUL_PATH and CONSUL_KEY are defined, the config files will be downloaded from $CONSUL_PATH/$CONSUL_KEY but the config file will be read from $CONSUL_PATH/config.ini\n  "
},
{
	"uri": "https://flokkr.github.io/docs/runtime/compose/",
	"title": "Docker compose based hadoop/spark cluster",
	"tags": [],
	"description": "",
	"content": " The https://github.com/flokkr/runtime-compose repository contains example configuration to run various type of clusters (eg. Hadoop HA, Hadoop federation, Spark, etc.)\nUsually the could be started with\ndocker-compose up -d  To scale services you can run\ndocker-compose scale datanode=1  But please note that not all the containers can be scaled up. The master components (such as Hadoop namenode) usually have hardcoded hostnames which avoid the scaling,\nCommon properties    Topic Solution     Configuration management    Source of config files: docker-compose external environment variable file   Configuration preprocessing: envtoconf (Convert environment variables to configuration formats   Automatic restart on config change: Not supported, docker-compose up is required   Provisioning and scheduling    Multihost support NO   Requirements on the hosts docker daemon and docker-compose   Definition of the containers per host N/A, one docker-compose file for the local host   Scheduling (find hosts with available resource) NO, localhost only   Failover on host crash NO   Scale up/down: Easy with docker-compose scale datanode=3   Multi tenancy (multiple cluster) Partial (from multiple checkout directory, after port adjustment)   Network    Network between containers dedicated network per docker-compose file   DNS YES, handled by the docker network   Service discovery NO (DNS based)   Data locality NO   Availability of the ports Published according to the docker-compose files    "
},
{
	"uri": "https://flokkr.github.io/docs/",
	"title": "Docker images for Open Source bigdata/hadoop projects",
	"tags": [],
	"description": "",
	"content": " Flokkr is an umbrella github organization to collect all of my containerization work for Apache bigdata/datascience projects such as Apache Hadoop or Apache Spark.\nOn high level, there are two main type of the subprojects/git repos under this organization: Containers and runtime configuration examples.\nIf you would like to run a simple Apache bigdata project, open the repository and use the included docker-compose file. If you need a more sophisticated cluster which includes multiple product and different configuration: investigate the runtime repositories and choose a method which is the most appropriate for you.\nContainers All of the containers are based on one smart baseimage defined in flokkr/docker-baseimage. It contains all the configuration loading script (based on environment variables or consul servers) and other extensions (eg. btrace instrumentation).\nTo get more information about the available environment variables check the flokkr/launcher repository.\nAll the other containers can be found with docker- prefix under the flokkr organization.\nThe containers are usually built on travis-ci and pushed to the docker hub instead to use dockerhub automatic buidls due to the limitation of the dockerhub (for example it\u0026rsquo;s hard to generate matrix builds with all the older versions).\nAvailable images:\n   Repository Product     docker-baseimage Base image with all the configuration loading magic   docker-hadoop Apache Hadoop components (hdfs/yarn)   docker-spark Apache Spark components   docker-storm Apache Storm components   docker-zookeeper Apache Zookeeper components   docker-kafka Apache Kafka components   docker-hbase Apache HBase components   docker-zeppelin Apache Zeppelin interface   docker-krb5 Highly insecure kerberos container, with an open REST api to request new kerberos keytab files.    Note: previous version of the containers (and some not yet migrated) can be found under the github.com/elek account.\nRuntime examples Docker image creation is easy, just a few lines to download and unpack the Apache projects. The tricky part is how the containers could work together: service discovery, configuration management, data locality, multi-tenancy, etc.\nThere are various examples how the containers could be used and each of them have a separated repository with the runtime- prefix.\n   Repository Details     runtime-compose docker-composed based pseudo clusters (multiple containers but only for one hosts). Configuration are defined by environment variables. For development and local experiments.   runtime-consul Multi-host real cluster with consul (for storing the configuration and docker-compose definitions) and docker-compose. Small scripts help to maintain the cluster state (restart components on every config change). Full data-locality is achieved by using docker host network.   runtime-nomad Multi-host real cluster with consul (for storing the configuration and docker-compose definitions) and nomad (to start the instances). Small scripts help to maintain the cluster state (restart components on every config change). Full data-locality is achieved by using docker host network.   runtime-swarm Similar to the previous one, but the container scheduling part is simplified with docker-compose + swarm. No host network, so no data-locality. Environment variable based configuration management.   runtime-kubernetes Kubernetes managed cluster with kubernetes ConfigMap based configuration set.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/",
	"title": "Flokkr launcher",
	"tags": [],
	"description": "",
	"content": "The base docker images (and after that every docker image) contains a simple script to launch the command (docker entrypoint).\nThe source of this launcher script is maintained in the launcher repository.\nThe launcher scripts countains multiple conditional script fragment which could be activated by environment variables. These fragments provide additional functionality to the base image. For example:\n Could convert environment variables to configuration files Could instrument java applcation with btrace agent Could retry the execution Could read configuration from consul server Could sleep for a specific time  "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/installer/",
	"title": "Installer plugin",
	"tags": [],
	"description": "",
	"content": "Installer plugin could replace built in components\nThe original products usually unpacked to the /opt directory during the container build (eg. /opt/hadoop, /opt/spark, etc\u0026hellip;). The install plugin deletes the original product directory and replaces it with a newly one downloaded from the internet.\n   Name Default Description     INSTALLER_XXX  The value of the environment variable should be an url. If set, the URL will be downloaded and untar-ed to the /opt/xxx directory. For example set INSTALER_HADOOP=http://home.apache.org/~shv/hadoop-2.7.4-RC0/https://home.apache.org/~shv/hadoop-2.7.4-RC0/hadoop-2.7.4-RC0.tar.gz to test an RC.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/kerberos/",
	"title": "Kerberos plugin",
	"tags": [],
	"description": "",
	"content": " Kerberos plugin downloads/generates kerberos keytabs and ssl key/truststore\nOur total UNSECURE kerberos server contains a REST endpoint to download on-the-fly generated kerberos keytabs, java keystores (ssl keystores, trustores). This plugin could be configured to download the files. The plugin also copies krb5.cfg to /etc.\nConfiguration    Name Default Description     KERBEROS_SERVER krb5 The name of the UNSECURE kerberos server where the REST endpoint is available on :8081   KERBEROS_KEYTABS  Space separated list of keytab names. With every element a new keytab will generated to $CONF_DIR/$NAME.keytab with a key for $NAME/$HOSTNAME@EXAMPLE.COM.   KERBEROS_KEYSTORES  Space separated list of certificate names. For every name a new keystore file will be generated to the $CONF_DIR/$NAME.keystore which contains a key for cn=$NAME. Trust store will also be generated to $CONF_DIR/truststore.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/retry/",
	"title": "Retry plugin",
	"tags": [],
	"description": "",
	"content": " The plugin tries to run the entrypoint of the image multiple times. If the process has been exited with non zero exit code, it tries to rerun the command after a sleep. The sleep time increasing with every iteration and the whole process will be stopped anyway after a fix amount of retry. If the process run enough time (60s) the failure counter and sleep time is reseted.\nConfiguration    Name Default Description     RETRY_NUMBER 10 Number of times the process will be restarted (in case of non-zero exit code   RETRY_NORMAL_RUN_DURATION 60 After this amount of seconds the RETRY_NUMBER counter will be reseted. Example: After 5 tries the process is started and run successfully 5 minutes. After a non-zer exit, it will be rerun RETRY_NUM (10) times. Example 2: After 5 tries the process is starts, runs for 40 seconds, and exits. The retry will continue with the reamining 5 try.    "
},
{
	"uri": "https://flokkr.github.io/docs/launcher/sleep/",
	"title": "Sleep plugin",
	"tags": [],
	"description": "",
	"content": " SLEEP: sleep for a specified amount of time.    Name Default Description     SLEEP_SECONDS  If set, the sleep bash command will be called with the value of the environment variable. Better to not use this plugin, if possible.    "
},
{
	"uri": "https://flokkr.github.io/docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]